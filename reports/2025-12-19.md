# 19/12/25

spent this week doing a deep dive into HNSW implementation options. The main question is how do we add approximate nearest neighbor search while maintaining transaction safety with the existing LMDB graph store? this is really hard/interesting!!!

## HNSW overview

HNSW (Hierarchical Navigable Small World) is the current SOTA for approximate nearest neighbor search. it builds a multi-layer graph structure where:

1. Layer 0 contains all vectors, each connected to M neighbors
2. higher layers contain exponentially fewer nodes (each node has ~1/ln(M) probability of promotion)
3. search starts at the top layer, greedily follows edges toward the query, then descends to lower layers

higher layers act as "express lanes" - they let you skip large portions of the search space quickly, then refine in the dense bottom layer.

why it's fast:

- O(log n) average search complexity instead of O(n) brute force
- the "small world" property - even sparse random graphs have short paths between any two nodes. HNSW constructs the graph to maximize this property while keeping edges to geometrically nearby vectors
- cache locality for graph traversal - each node's neighbor *IDs* are stored contiguously, so one cache line fetch gets all M neighbor slots. the actual vector data for computing distances is stored separately and requires M random accesses to the vector file. ~95% of time is still spent in distance computation

## initial thoughts

My first thought was: we already have LMDB for the graph store, it provides MVCC transactions, why not just put the HNSW neighbor lists there too?

```
LMDB maps:
  hnsw_layers:    slot_id (8B) -> layer_count (1B)
  hnsw_neighbors: (slot_id, layer) (9B) -> neighbor_ids[] (variable)
```
and some other auxiliary maps

this would give us full ACID transactions "for free" - HNSW updates would be atomic with graph operations, readers would see consistent snapshots, crash safety via LMDB's CoW. but annoyingly this comes with a major downside after thinking more, and is a kind of lame/boring solution.

some things like single-writer and write amplification are inherent to copy-on-write MVCC regardless of implementation. but the B-tree structure specifically causes problems:

### cache locality destruction and lookup overhead

LMDB stores data in a B+ tree. a B+ tree is a balanced tree structure where:

- **internal nodes** contain only keys and pointers to children (no data)
- **leaf nodes** contain the actual key-value pairs
- tree is kept balanced so all leaves are at the same depth

for a database with N keys, the tree has depth O(log N). with a typical branching factor of ~100-500 (depending on key size and page size), a million-key database has depth 3-4.

dope diagram generated by claude:
```
looking up hnsw_neighbors[slot_id=12345]:

                    ┌─────────────┐
                    │  Root Page  │  ← load page, binary search for 12345
                    │ [keys 0-99] │     "go to child 3"
                    └──────┬──────┘
                           │
              ┌────────────┼────────────┐
              ▼            ▼            ▼
        ┌──────────┐ ┌──────────┐ ┌──────────┐
        │Internal 1│ │Internal 2│ │Internal 3│  ← load page, binary search
        └──────────┘ └──────────┘ └────┬─────┘     "go to child 7"
                                       │
                    ┌──────────────────┼───────────────────┐
                    ▼                  ▼                   ▼
              ┌──────────┐       ┌──────────┐        ┌──────────┐
              │  Leaf 6  │       │  Leaf 7  │        │  Leaf 8  │
              └──────────┘       │ 12345→[.]│        └──────────┘
                                 └──────────┘
                                       ↑
                              finally found the value!

total: 3 page loads, 3 binary searches, then read the value
```

compared to a flat array approach where we just do:
- one array index into page_table to get page offset - O(1)
- direct mmap access at computed offset
- total: 1 array lookup, 1 page load

the overhead difference:
- LMDB: ~300ns per lookup (3-4 cache misses for tree traversal)
- flat array: ~5ns per lookup (1 array index, usually cached)

this matters because HNSW search visits ~200 nodes. that's 60μs of overhead with LMDB vs 1μs with flat array.

### page fault penalty

when the dataset exceeds RAM, the B-tree structure is even worse:

| Access Pattern | Page Faults per lookup |
|---------------|-------------|
| Contiguous mmap | 1 (data page only) |
| LMDB B-tree | 3-4 (tree traversal) |

with 10% cache miss rate on a 200-node search, that's 60-80 extra page faults with LMDB. at ~100μs per SSD page fault, that adds 6-8ms to query latency.

## instead, custom MVCC with page-based copy-on-write

in my opinion, LMDB's problem here isn't copy-on-write itself, it's the B-tree structure only. we can build a custom storage layer that uses CoW for transactions but with a flat layout optimized for HNSW access patterns. this was also my original plan in the proposal but it's good to check/verify and make sure it's not an over complication.

### what's stored where

the HNSW index has two parts that need different treatment:

1. **vector data** (the float arrays) - stored in a separate mmap file, append-only, 32-byte aligned for SIMD. vectors are immutable once written, so no CoW needed here. the existing LMDB `vectors` map could also work for this (already implemented!) but a dedicated mmap file would give better alignment/cache behavior for distance computation.

2. **graph structure** (neighbor lists, layer counts) - this is what needs the custom MVCC. it's mutable (neighbors change when new nodes are inserted) and needs to be consistent with transactions.

so the custom MVCC file only stores the graph structure, not the vectors themselves. each "node" in the graph file is:

```
node structure (for M=16, max_layers=5):
  layer_count: u8            (1 byte)
  padding: 7 bytes           (alignment)
  layer0_neighbors: i64[16]  (128 bytes, M neighbors)
  layer1_neighbors: i64[8]   (64 bytes, M/2 neighbors)
  layer2_neighbors: i64[8]   (64 bytes)
  layer3_neighbors: i64[8]   (64 bytes)
  layer4_neighbors: i64[8]   (64 bytes)
  ---
  total: 392 bytes per node
```

neighbor slots are -1 if unused. slot IDs in neighbor lists reference other nodes in this same file.

### file layout

with 392-byte nodes, we get ~10 nodes per 4KB page.

```
┌─────────────────────────────────────────────────────────────┐
│ HEADER (4KB)                                                │
│   magic, version, M, max_layers, ef_construction            │
│   entry_point_slot: i64                                     │
│   node_count: u64                                           │
│   active_root: u64 (atomic pointer to current page table)   │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│ PAGE TABLE A (variable size, grows with dataset)            │
│   epoch_id: u64                                             │
│   page_count: u64                                           │
│   page_offsets: [u64; page_count]  <- O(1) array lookup     │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│ PAGE TABLE B - shadow table for writes                      │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│ GRAPH PAGES (append-only):                                  │
│   Page 0: [node0, node1, ..., node9]   <- ~10 nodes/page    │
│   Page 1: [node10, node11, ...]                             │
│   ...                                                       │
│   CoW pages appended here when modified                     │
└─────────────────────────────────────────────────────────────┘
```

the page table needs to grow with the dataset. for 1M vectors we'd need 100K pages, so the page table would be ~800KB. could either:

- allocate a big page table upfront (wastes virtual address space but that's cheap)
- or resize/reallocate when needed (more complex)

### how reads would work

- compute page index from slot: `page_idx = slot / nodes_per_page`
- look up page offset in page table: `page_table.offsets[page_idx]` - O(1) array index
- compute node offset within page: `offset + (slot % nodes_per_page) * node_size`
- read directly from mmap at that offset

### how writes would work

- compute page index from slot
- check if this page was already copied in this transaction (track in hashtable)
- if already copied: modify the copy in place
- if not: copy-on-write - copy the entire 4KB page to a new location, then modify

we copy at page granularity (~10 nodes per page), not per-node. multiple modifications to the same page reuse the copy.

note on neighbor locality: HNSW neighbors are close in *vector space*, but their slot IDs are assigned by insertion order - so neighbors are often scattered across different pages. worst case we copy 9 different pages for one insert (1 new node + 8 neighbors). locality helps somewhat if the dataset was built incrementally (similar vectors inserted around the same time), but shouldn't rely on it.

### how commit would work

- append all modified pages to end of file (new physical locations)
- update the shadow page table with new offsets for modified pages
- fdatasync to ensure data pages are on disk
- write shadow page table, fdatasync
- atomic 8-byte write to switch active_root pointer to shadow table
- fdatasync to ensure pointer update is durable

the commit point is the pointer swap. before that, readers see old version. after, readers see new version. crash at any point before the pointer swap leaves old version intact.

### MVCC for concurrent readers

multiple readers can run concurrently with one writer, readers snapshot the page table pointer at transaction start. they continue seeing that version even if a writer commits. this is exactly how LMDB works, but with our flat layout instead of a B-tree.

### garbage collection

since we append new pages on every write, old pages become garbage when no readers reference them. need to track which epochs have active readers (reference counting) and reclaim old pages when their epoch has no readers. can either:

- maintain a free list of reclaimable page offsets
- periodically compact the file (copy live pages, truncate)
- or just let the file grow and compact on restart

for MVP, letting it grow and compacting on restart is probably fine.

### write amplification

HNSW insert modifies 1 new node + ~8 neighbors (at M=16, assuming ~50% full neighbor lists), so:

- worst case (all in different pages): 9 pages x 4KB = 36KB
- best case (neighbors happen to share pages): fewer pages, maybe 12-20KB
- LMDB would be 40-80KB per insert due to B-tree internal node updates

realistically, since slot IDs are assigned by insertion order and neighbors are geometrically close (not insertion-time close), expect closer to worst case for random workloads. still ~2-3x less write amplification than LMDB because we don't have B-tree overhead.

## comparison

| property | raw mmap | custom MVCC | LMDB |
|----------|---------|------------|------|
| graph lookup overhead | 0 | ~5ns | ~300ns |
| transaction isolation | nope | yep | yep |
| crash safety | nope | yep | yep |
| write amplification | 1x | 2-3x | 7-10x |

## why SIMD alignment matters for vectors

distance computation (euclidean, cosine, dot product) is the hot path in HNSW, takes ~95% of search time. modern CPUs have SIMD instructions that process multiple floats in parallel:

- SSE: 4 floats at once (128-bit), requires 16-byte alignment
- AVX: 8 floats at once (256-bit), requires 32-byte alignment  
- AVX-512: 16 floats at once (512-bit), requires 64-byte alignment

for a 768-dim vector, AVX can compute euclidean distance in ~96 iterations instead of 768. but if the vector isn't 32-byte aligned, the CPU either:

- faults (with aligned load instructions)
- or falls back to slower unaligned loads (still works but slower)

LMDB doesn't guarantee alignment, values go wherever they fit in B-tree pages. a 3072-byte vector might start at any byte offset. so we need a separate mmap file for vectors where we control the layout.

the actual SIMD code would be in distance computation, either:

- C stubs calling intrinsics (`_mm256_load_ps`, `_mm256_fmadd_ps`, etc)
- or is there some way to do this in ocaml???

## integration with LMDB

the plan is to use three storage systems:

- **LMDB**: graph data (nodes, edges, adjacency), vector metadata (`vector_owners`, `vector_index`), slot mappings (`hnsw_v2s`, `hnsw_s2v`)
- **vectors.bin** (append-only mmap): raw float32 vectors, 32-byte aligned slots, immutable once written
- **hnsw_graph.bin** (custom MVCC): HNSW neighbor lists and layer counts

the existing LMDB `vectors` map gets removed

### create_vector flow

- begin LMDB transaction
- allocate new vector_id from LMDB counter
- append vector data to vectors.bin (get vector_offset)
- store vector metadata in LMDB (vector_id -> node_id, tag, vector_offset)
- begin HNSW write transaction
- insert into HNSW graph (allocate slot, find neighbors, update neighbor lists)
- store slot mapping in LMDB (vector_id <-> slot)
- commit HNSW transaction
- LMDB transaction commits on return

### crash safety

- crash before vectors.bin append: nothing written, clean state
- crash after vectors.bin but before HNSW commit: orphan vector data in vectors.bin (cleaned up on recovery by checking against LMDB)
- crash after HNSW commit but before LMDB: orphan slot in hnsw_graph.bin AND orphan vector in vectors.bin (both cleaned up on recovery)
- crash after LMDB commit: fully consistent state

## next steps

- implement this!!!
- write tests for MVCC correctness (concurrent readers, crash recovery)
- benchmark against brute force and hnswlib to make sure we're not leaving performance on the table
