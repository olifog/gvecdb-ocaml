# 09/01/26

this update brings a large refactor/cleanup and an initial version of the mmaped append-only vector store.

notes:

- io_uring is cool and faster than mmap - for the purpose of getting an MVP done decided to stick with mmap for now for hnsw/vectors (also because lmdb is mmaped and it feels right to keep the interface the same for the 3 stores)
- decided to split out hnsw implementation into phases:
  1. extracting out vectors from lmdb into a mmap-ed append-only vector store (<---- done in this commit)
  2. implementing an in-memory hnsw index from the mmap-ed vector store
  3. making this hnsw index persistent on disk (not safe yet)
  4. custom copy-on-write MVCC
  5. make sure concurrent readers is all good with like epochs (need to read/think here a bit more)
  6. (extension-ish) garbage collection of the hnsw index pages and of old unused vector entries in the vector store
  7. (extension-ish) SIMD, i put effort into making the vector stuff all 32-byte aligned for this
- while doing this first phase, I also put a bit of effort into optimising the brute-force knn (e.g. using a windowed maxheap, reducing unneeded computation), there were a bunch of inefficiencies - this isn't important really but makes the benchmark comparisons more realistic/interesting
- I couldn't figure out how to ca# 09/01/26

this update brings a large refactor/cleanup and an initial version of the mmaped append-only vector store.

notes:

- io_uring is cool and faster than mmap - for the purpose of getting an MVP done decided to stick with mmap for now for hnsw/vectors (also because lmdb is mmaped and it feels right to keep the interface the same for the 3 stores)
- decided to split out hnsw implementation into phases:
  1. extracting out vectors from lmdb into a mmap-ed append-only vector store (<---- done in this commit)
  2. implementing an in-memory hnsw index from the mmap-ed vector store
  3. making this hnsw index persistent on disk (not safe yet)
  4. custom copy-on-write MVCC
  5. make sure concurrent readers is all good with like epochs (need to read/think here a bit more)
  6. (extension-ish) garbage collection of the hnsw index pages and of old unused vector entries in the vector store
  7. (extension-ish) SIMD, i put effort into making the vector stuff all 32-byte aligned for this
- while doing this first phase, I also put a bit of effort into optimising the brute-force knn (e.g. using a windowed maxheap, reducing unneeded computation), there were a bunch of inefficiencies - this isn't important really but makes the benchmark comparisons more realistic/interesting
- I couldn't figure out how to call an msync to flush mmap stuff after vector creates/updates from within ocaml, so linked in a c stub in I think the correct way? it seems to be working - some other libraries probably expose msync bindings within themselves though, might be better to use that if possible
- linked to the above is that a side effect of this is that windows support is now dropped - msync is a posix thing, I can probably put a bit more work in to detecting the system and using the windows equivalent later down the line
- important - knn and hnsw should be per-vector tag. isolated
- one thing I neglected to think about last time, which came up when I was planning out using this database within another personal project, is that knn and hnsw should be isolated per-vector tag. doing a global vector closeness search on all vectors, regardless of tag, is not a good devex - can filter by tag as a post-step but this is pretty inefficient. Instead, as I implement the in-memory and persistent hnsw indexes, I'll need to make sure this is architected such that there are multiple, one for each tag
- I got an awesome new linux laptop, and while setting up this repo on it i realised the readme was rather lacking - fixed!
- as soon as this hnsw in memory index is done benchmarking is number 1 prio, for raw graph operations too
- vectors need headers! can pre-normalize for cosine, stored norm precomputed (used for all distance metrics - euclidean early rejection, cosine is just dot product on normalized vecs, dotproduct needs to scale back up). this may be a premature optimisation, once benchmarks in place can remove this if it doesnt matter too much
- added support for edge vectors! vectors can now attach to edges not just nodes. used a packed int64 encoding where bit 63 is the type flag (0=node, 1=edge) and the rest is the id. not sure how I didn't think about this beforehand, this feels like a pretty crucial oversight from earlier in the project !!!

## vector file format

the `.vectors` file that sits alongside the lmdb `.db` is formatted:

```
┌─────────────────────────────────────────────────────────────┐
│ FILE HEADER (32 bytes)                                      │
│   magic: "GVECVECS" (8 bytes)                               │
│   version: int64 (currently 3)                              │
│   slot_size: int64 (reserved, unused for now)               │
│   next_offset: int64 (bump pointer for next allocation)     │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│ VECTOR ENTRIES (each 32-byte aligned)                       │
│                                                             │
│   vector header (16 bytes):                                 │
│     dim: int32 (number of float32 elements)                 │
│     flags: 1 byte (bit 0 = is_normalized)                   │
│     reserved: 3 bytes                                       │
│     norm: float64 (original L2 norm before normalization)   │
│                                                             │
│   vector data: dim * 4 bytes (float32 little-endian)        │
│   padding: to next 32-byte boundary                         │
│                                                             │
│   ... more vectors ...                                      │
└─────────────────────────────────────────────────────────────┘
```
nice diagram courtesy of claude!

it's append-only, allocation bumps the next_offset forward and returns the old value, with the file doubling in size when needed. the 32-byte alignment is for future SIMD - AVX works on 32-byte aligned stuff only (I think!)
the slot_size field in the header is reserved for potential future fixed-size slot mode (would make gc easier) but not using it currently
ll an msync to flush mmap stuff after vector creates/updates from within ocaml, so linked in a c stub in I think the correct way? it seems to be working - some other libraries probably expose msync bindings within themselves though, might be better to use that if possible
- linked to the above is that a side effect of this is that windows support is now dropped - msync is a posix thing, I can probably put a bit more work in to detecting the system and using the windows equivalent later down the line
- important - knn and hnsw should be per-vector tag. isolated
- one thing I neglected to think about last time, which came up when I was planning out using this database within another personal project, is that knn and hnsw should be isolated per-vector tag. doing a global vector closeness search on all vectors, regardless of tag, is not a good devex - can filter by tag as a post-step but this is pretty inefficient. Instead, as I implement the in-memory and persistent hnsw indexes, I'll need to make sure this is architected such that there are multiple, one for each tag
- I got an awesome new linux laptop, and while setting up this repo on it i realised the readme was rather lacking - fixed!
- as soon as this hnsw in memory index is done benchmarking is number 1 prio, for raw graph operations too
- vectors need headers! can pre-normalize for cosine, stored norm precomputed (used for all distance metrics - euclidean early rejection, cosine is just dot product on normalized vecs, dotproduct needs to scale back up). this may be a premature optimisation, once benchmarks in place can remove this if it doesnt matter too much
- added support for edge vectors! vectors can now attach to edges not just nodes. used a packed int64 encoding where bit 63 is the type flag (0=node, 1=edge) and the rest is the id. not sure how I didn't think about this beforehand, this feels like a pretty crucial oversight from earlier in the project !!!

## vector file format

the `.vectors` file that sits alongside the lmdb `.db` is formatted:

```
┌─────────────────────────────────────────────────────────────┐
│ FILE HEADER (32 bytes)                                      │
│   magic: "GVECVECS" (8 bytes)                               │
│   version: int64 (currently 3)                              │
│   slot_size: int64 (reserved, unused for now)               │
│   next_offset: int64 (bump pointer for next allocation)     │
└─────────────────────────────────────────────────────────────┘
┌─────────────────────────────────────────────────────────────┐
│ VECTOR ENTRIES (each 32-byte aligned)                       │
│                                                             │
│   vector header (16 bytes):                                 │
│     dim: int32 (number of float32 elements)                 │
│     flags: 1 byte (bit 0 = is_normalized)                   │
│     reserved: 3 bytes                                       │
│     norm: float64 (original L2 norm before normalization)   │
│                                                             │
│   vector data: dim * 4 bytes (float32 little-endian)        │
│   padding: to next 32-byte boundary                         │
│                                                             │
│   ... more vectors ...                                      │
└─────────────────────────────────────────────────────────────┘
```

diagram courtesy of claude!

it's append-only, allocation bumps the next_offset forward and returns the old value, with the file doubling in size when needed. the 32-byte alignment is for future SIMD - AVX works on 32-byte aligned stuff only (I think!)
the slot_size field in the header is reserved for potential future fixed-size slot mode (would make gc easier) but not using it currently
