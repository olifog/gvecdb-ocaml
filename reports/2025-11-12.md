# 12/11/25

LMDB key structure and graph data model

For the graph portion of the database, we will need to decide on the set of indexes the db should handle. each of these is a separate b+ tree of string keys matched to binary blobs within LMDB, and we can use compound keys for fancy range scans. The most naive approach would be to have a node index (node id -> binary blob of node data), an edge index (edge id -> binary blob of edge data), and an adjacency index (source node id, target node id -> edge id). there's a lot of choices/flexibility here though, especially when considering more stuff like edge/node properties, edge directionality, database metadata like versioning/next id tracking, etc.

I think the easiest way to frame this is going through a bunch of individual design choice points and reasoning about them, to yield what the overall design should look like. General design goals (as outlined in my OG proposal) are:
- read-heavy workloads
- high-throughput
- embedded deployments, i.e. working set normally fits in memory, DB not too large

1. ID space/key width

need to decide u64/u32/u128 etc for IDs - u64 probably best compromise between range and storage size

2. integer endianness/sort semantics

big endian for integer keys makes lexicographic order == numeric order, prefix/range queries in the b+ trees will be easier to reason about and implement

3. record format for nodes/edges data blobs

bunch of different options here:
- JSON string (evil)
- string <> Value hashmap
- binary struct, enforce a strict schema for node/edge type properties

need to discuss this with supervisor! i think going the fully typed properties route would be cool and allows super neat flexible indexing in future, but makes the database a lot less flexible/usable. could argue that just having a JSON string blob key is a decent enough escape hatch from the binary struct route so could be okay, but adds more questions around a type system etc.

another thing to think about here is hot vs cold properties, and how to handle super large property values. it's probably suboptimal to have a massive property directly in the record blob for a node/edge, instead normalize it out elsewhere to make the primary node/edge index more performant (keep it fitting within memory). normalizing it out could just be as simple as an extra b+ tree index matching property id to property value blob and another index coupling node ids with property ids via a compound key

this would be great to talk about!!

4. adjacency index shape

a few options here, but either way i think it would be good to have a redundant outbound/inbound index.

option 1, just link edges to nodes:
outbound: `(srcNodeId, edgeId)`
inbound: `(dstNodeId, edgeId)`

option 2, do something slightly more complex by embedding edge type and opposing node id in the key structure:
outbound: `(srcNodeId, typeId, dstNodeId, edgeId)`
inbound: `(dstNodeId, typeId, srcNodeId, edgeId)`

option 1 keeps life simple and the keys are smaller, but option 2 is more performant for adjacency queries that are constrained by edge type - option 1 would have to have edge type hop filters as a secondary step after the initial lookup. option 2 also allows pretty neat fast queries for all edges of a given type between two nodes.

5. separate edge storage vs just being in adjacency index(es)

one thing to at least consider is if it's actually worthwhile to embed the edge data directly in the adjacency index(es)? it would add some redundancy (and thereby code complexity) but would reduce the number of indexes and maybe some other code paths?

6. separation of nodes and edges as concepts vs everything being a node

one other kind of wild idea is just get rid of the node/edge distinction altogether (at least in the underlying store schema) and have everything be a node with a type and properties/linked vectors. edge/node distinction is just a semantic/logical distinction, boolean flag or something on the node would be the indication, and adjacency is just a property between two nodes. this may simplify the codebase and make the common path pretty fast, but it's unclear if this would actually be useful or just make things more confusing/eliminate some future flexibility/features.

this is sort of analogous to the AWS Neptune style quads/SPOG way of doing things https://docs.aws.amazon.com/neptune/latest/userguide/feature-overview-data-model.html

7. label/type dictionaries, hashing or not

users will refer to labels and types of edges/nodes as strings, but internally we'll store them as IDs. could either have a metadata entry in an index that increments up the next available ID monotonically, or hash the string to a u64/u32/u128 id. no idea what's better here/faster - maybe try both and have an option to toggle? then run benchmarks on millions of writes?

8. vector adjacency representation

vectors themselves will be stored in the separate HNSW slab world, but we still need some way to associate any number of vectors (and their vector type) with a certain node/edge. could just be an index of the form `nodeId, vectorTypeId -> vectorId`?

9. migration/versioning support

this is fairly straightforward to implement if we have a general database metadata index that matches a known metadata ID to an int, one of these metadata IDs could be used to track the current version of the database being used

migration of data during updates to new version of the database would need some form of auto-migration script that can be run on each new database release, that operates on the last previous version of the database and migrates it to the new one (in case there are breaking internal changes in e.g. internal LMDB key structures). old old versions can be updates by induction, apply each migration script in order

10. ID generation

next available ID could be another metadata item in the metadata index, or random? or a hash of time? or all of the above mixed? again maybe worth trying all and seeing what works

11. big one (should have put this higher) but strongly typed node property schema support or not

i sort of talked about this in the record format (3.) but if i do decide to go the route of forcing the user to strongly type their node/edge properties based on node/edge type, we need some way to store this schema in the database and then validate writes. would need to figure out a schema format/DSL. it could have immense benefits though for speed and cool indexing features.

one option here is being capnproto maximalist and have database clients/users define their node/edge schemas in capnproto schemas, with comments denoting whether a certain prop should be stored hot/cold (with sane defaults for certain types?) since the eventual record is literally a binary struct stored in lmdb this schema could even be used to directly deserialize/serialize the records after/before they've been read/written to lmdb. could be super fancy here and have zero copy all the way down, since the mmap combined with capnproto zero copy would be dope - requires capnproto schema compilation internally though??? this could get confusing
