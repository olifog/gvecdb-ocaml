# 27/11/25

This week I went back and audited whether the "zero-copy" stuff from last week was actually zero-copy. it wasn't!!! spent most of this week fixing that and then expanded test suite + added CI and a transaction api.

## zero-copy problems

found a bunch of issues:

1. it was using `Lmdb.Conv.string` which copies from mmap into ocaml heap strings. should use `Lmdb.Conv.bigstring` which gives you a view directly into mmap
2. had a wrapper schema (`gvecdb_internal.capnp`) that wrapped user properties with metadata - which actually meant double deserialization, double the copies
3. O(n^2) serialization with string concat in a loop (it's been a while since FoCS)
4. even after fixing lmdb, capnp-ocaml's `FramedStream.of_string` needs an ocaml string so there was still a bigstring->string copy

## first round of fixes

- switched lmdb maps to `Lmdb.Conv.bigstring`
- killed the wrapper schema - split metadata into separate tables (`node_meta`, `edge_meta`) with compact binary (8 bytes for node type, 24 bytes for edge type+src+dst)
- rewrote serialization to two-pass: measure size, allocate once, blit

got down to one copy, the bigstring->string for capnp-ocaml

## next round, actual zero-copy read path

capnp-ocaml is functorized over message storage! from their readme:

> The generated code is functorized over the underlying message format. This enables client code to operate equally well on messages stored as—for example—OCaml bytes buffers or OCaml Bigarray (perhaps most interesting for applications involving mmap()).

so we can make a `BigstringMessage` backed by bigstrings instead of bytes. had to implement `Capnp.MessageStorage.S` and then:

```ocaml
include Capnp.Message.Make(Bigstring_storage)
```

With `BigstringMessage`, the read path becomes:

1. lmdb lookup -> bigstring pointing to mmap (zero-copy)
2. parse wire format header -> read a few ints for segment sizes
3. `Bigstring.sub` for views -> these are views not copies
4. wrap in `BigstringMessage.Message.of_storage`
5. capnp reader reads directly from mmap

for int/struct fields this is truly zero-copy. text/data fields still copy annoyingly because capnp-ocaml returns ocaml strings on the heap, I'm not really sure if there's a way to avoid this (or whether there should be). One thing to consider here either way is that mmap-ed bigstring values are only valid within a transaction, after a transaction executes those memory regions are not guaranteed to be valid anymore. so perhaps it's for the best

## verifying it works

My initial test used `Gc.stat().heap_words` which was misleading - showed "0 bytes" even when copies were happening. I switched to `Gc.allocated_bytes()` in a sample test, where I measured reading a node of size 5MB, 1MB of which being the property `bio`, with `age` being a basic int.

```
BytesMessage:
  age access: 5,002,976 bytes  (copies entire 5MB message)
  bio access: 6,003,400 bytes  (copies entire message and then copies the string again)

BigstringMessage:
  age access: 1,520 bytes      <- actually zero-copy
  bio access: 1,001,944 bytes  (just the 1MB string field)
```

This proves (hopefully) that this is actually zero-copy this time! the 1520 bytes are small reader/message wrapper overhead, allocating BigstringMessage.Message struct, and is fixed overhead regardless of the message size

## next tasks (not in order, not all necessary for mvp or even final project, listing for completeness)

- vectors/HNSW slab stuff (over christmas)
- patch operations on properties
- benchmarks
- (important!!) figure out the exact way for users to supply their edge/node schemas to a cli tool that then takes the schemas and generates/builds a user-specific database binary for them, exposing an api over capnproto and generating a full capnp schema for them to generate language-agnostic client code for
- filter adjacency queries by properties
- need to expose some way to scan all nodes/edges/vectors in the database. could do this be adding indexes on node/edge type ids, would also allow operations like "get all nodes of type X"
- need cascade in node deletion to connected edges
- the lmdb map size is currently hard coded, should be a parameter to create (and auto increase?)
- need to document more heavily (or think harder) about the memory safety implications of mmap-ed bigstrings, making sure users don't accidentally use them after a transaction has committed
- error handling is inconsistent, some functions return options and some raise Failure, some raise Lmdb exceptions
- scan_adjacency_index executes O(N) LMDB lookups (N being the number of edges from the node) which seems bad, maybe can use index more efficiently? or add another index if needed?
- schema validation on writes, currently registering a schema is completely useless because it's never checked
- need to test database reopening, persistence across close/open
- batch operations
- graph algorithms would be cool, pathfinding/centrality etc
- do deepdive into whether zero-copy actually matters for use cases. based on initial testing/reasoning I think there's only savings when a node is of size > ~2Kb. this super depends on usecase - for small social graphs the added complexity for zero-copy wont be worth it, the overhead will dominate and reduce performance. for graphs with large nodes, e.g. a property storing the full text of a research paper, and/or selective access of properties, or high-frequency access to primitive properties, zero-copy will be a win
