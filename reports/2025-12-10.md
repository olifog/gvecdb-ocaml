# 10/12/25

## initial vector implementation with brute-force knn

Got the initial vector storage working, putting directly into lmdb maps with two auxiliary maps for linking vectors to their owning node and tag.
for now went with 3 map structures:

1. `vectors`: `vector_id -> vector_data` (raw float32 bigstring)
2. `vector_index`: `(node_id, vector_tag_id, vector_id) -> empty` - compound key for range scans getting all vectors for a node, optionally filtered by tag
3. `vector_owners`: `vector_id -> (node_id, vector_tag_id)` - reverse lookup to find owning node and tag given a vector_id

then wrote a super simple brute-force knn implementation that scans the entire vector map, computes euclidean/cosine/dot product distances, and returns the k nearest to a given query vector. right now there's
no defining the dimensions of a certain vector tag, so query vectors aren't validated for correct length (and you can write whatever you want to the vector map) - need to think about best way to do this! given the rest of the database's focus on type + memory safety with the capnproto stuff we should definitely not leave this as a footgun

a lot of this (including the core 3 map structures) will probably need to be refactored when the hnsw slab stuff is implemented, this is probably the next major task.

## result type refactoring

One big thing wrong with the code was various option types being mixed with result and exceptions/error types. this was a pain to handle and debug and had a ton of holes where error handling was completely missing. switched to using results everywhere and `let*`/`let+` for chaining results, along with some well defined error types and small utility functions.

## small fixes

1) fixed the O(N) issue described in 28-11-25, now extracting edge src/dst directly from the composite keys with no need for a map and second edge fetch
2) thought more about it and im 99% sure we won't need a batch api. internally it's all memory mapped so batching isn't important, and over the wire we'll be using promise pipelining and network requests will be batched up at the capnp client level. in certain edge cases (e.g. migrating a full database from one server to another) we might need a batch api, but that's not worth considering for this project i think!
3) thought more about the hard coded map size, there were 4 options:
   1) large default (e.g. 10GB)
   2) user supplied parameter
   3) auto resize on Map_full
   4) large default + user override
virtual address space is super cheap, the actual memory is only allocated when needed so can safely go with 4 i think! treating it like an upper bound on the actual database size on disk. resize on Map_full is a little bit of a pain because it can't be done while any transaction is ongoing, wasnt sure if dealing with that was worth the complexity

## next stuff!

- still need to work on the standalone binary/capnp interface generator
- then will write a design doc/plan for the hnsw slab stuff. properly formalise the plan and what it should all look like
- and then implementing hnsw!!
- after all that is done, can go into the other stuff mentioned in 28-11-25 - i.e. filter adjacency queries by properties, patch operations on properties, and graph algorithms (pathfinding/centrality etc)
- then can start on the extensions. i think first task should be making the research paper graph usecase with a nice demo, this will hopefully also reveal any errors/bugs/missing features/etc