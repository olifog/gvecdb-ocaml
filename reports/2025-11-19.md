# 19/11/25

Points from supervisor meeting:

## Some example use cases and desired performance characteristics would be super helpful!!!

Some example use cases, contrived but hopefully illustrative/realistic:

- embedded within a local knowledge graph application (e.g. obsidian, emacs org-mode, etc). Would serve as a source of truth for individual notes and their links/backlinks, with note content being embedded as vectors linked to note nodes. These vectors would serve as the index for semantic search, and would be able to recommend links/backlinks on the fly and do other fancy stuff. Working set would be worst case ~10k-100k notes (nodes), with maybe 3-5x as many edges for links/backlinks/semantic connections. Read-heavy workload (99:1 read:write ratio?) since notes are queried/searched way more often than created/updated. Writes are small and localized (single note + its immediate neighbors). Main resource constraint is keeping the whole working set in memory on a typical laptop (8-16GB RAM available, db should stay under 1-2GB worst case), with low latency for semantic search queries (<100ms p99) since it's interactive and user-facing
- deployed as a sidecar on a server with a web application that allows exploration + search of academic papers and individuals. papers/journals/authors would be individual node types with edges between them for citations/references/authorship/publishing relationships etc., with multiple vectors linked to each node for different purposes (e.g. title, abstract, full text, etc). This datasource would be incrementally updated as new papers/authors/etc are pulled from external sources on a cron job. Working set here would be substantially larger, maybe 1M-10M papers with 5M-50M citation edges, plus authors/journals/etc. Still read-heavy but more balanced (90:10 read:write?), with bulk writes happening during the cron job updates (batch inserts of thousands of new papers/citations at once). Resource constraints are less strict since it's server-side, maybe 32-64GB RAM available with db size up to 10-20GB in memory. Query latency can be slightly higher (<500ms p99) since it's not as interactive, but throughput needs to be decent to handle concurrent users (100+ QPS for reads, 10+ QPS for writes during updates)

## pros/cons of keeping it as a library that you link in vs a standalone binary

- devops/building/packaging etc out of scope and boring for part ii project
- linking it in means we can do more fancy capnproto stuff potentially, have the user of the library handle the schema management and serialization/deserialization + generation of getters and setters
- this is what irmin does!

## migration/versioning support is not important to think too hard about right now

The extent of migration/versioning support I'll think about up front is putting in a version number in the database metadata index. migration scripts/handling breaking property schema changes is not necessary for the mvp (and arguably not even necessary for the final project)

## strongly typed schemas vs string <> value hashmap for node/edge properties

this is a big one! The overall conclusion here is to not prematurely optimize for secondary indexes/performance benefits, but at the same time don't sacrifice too much performance for zero benefit. with that in mind, let's ignore cold props vs hot props for now and just put everything in hot props in the primary record blob. generally, if we strongly type the properties for nodes/edges with a schema (capnproto would be awesome here) writing/reading properties will be faster, and secondary indexes will be more efficient to update. there are also large benefits in situations where node property sizes are large (since we're not using cold props) - using an untyped hashmap would require copying the whole property value into memory even if you just need to read one field.

existing graph/vector databases generally fall into 3 categories with regards to property schemas:

1. schema-first + strongly typed fields - TigerGraph, JanusGraph, Dgraph, Weaviate, Milvus, Vespa, pgvector/Postgres (since it's postgres!!!!). Properties/metadata are declared in a schema with explicit types, the engine can leverage types for indexing, query planning, and validation. types aren't stored alongside values since the properties are strongly typed
2. schema-optional + typed values at runtime - Neo4j, memgraph, Neptune, ArangoDB. there aren't necessarily defined schemas for a certain node/edge type, but stored values are stored with their types and indexing isn't much harder
3. schemaless with limited primitive types - Pinecone, Qdrant, Chroma. great for flexibility but runtime complexity is higher for accessing properties + validation + secondary indexing

I strongly lean towards option 1 and giving the capnproto method a go. I think the work needed to get a hybrid option 2 working, or even a basic option 3 working with hashmaps is going to be a lot of work, and I might as well put that work towards the more robust solution that fits the use cases better. Performance (latency/throughput) are the main goals of the db, not flexibility, and option 1 suits that best

## maybe string interning should just be hashes? instead of storing strings in the database and mapping back to ids?

we need to intern strings! node types, edge types, property names (?)/values (?), etc.
when the user asks for all nodes of type "person", this should be mapped to an internal id for that node.

3 design decisions here:

1. how are IDs assigned/generated? just hash the string? Sequential ints?
   - hashing the string is cheap and easy, with no need to maintain a central registry for next id. there is a non-zero chance of collisions though, miniscule but still worth thinking about. locality would also be really bad if using an LMDB B+ tree since it's entirely random order
   - sequential IDs would have way better locality, new IDs are clustered, and iterating strings by range would be a possible operation. it needs a registry to track the next id, but I think this is doable and not much overhead, worth it
2. where do the mappings live? do we use an LMDB index? or a separate in-memory hashmap?
   - since we're going with these IDs being sequential ints, and we need to store this mapping on disk alongside the actual data, putting it directly in LMDB makes the most sense. there will be forward and reverse indexes for the mappings, and a metadata index for the next id to assign.
3. which strings do we actually intern?
   - the only ones i can think of are node types, edge types, property names (maybe - depending on how we do the capnproto stuff with properties), and property values for string props - again with a big question mark here based on how I do props

## what are the downsides of LMDB?

- it's a single writer database, so never able to do huge write-centric operations or support write-heavy workloads.
- mmap is pretty bad according to https://db.cs.cmu.edu/papers/2022/cidr2022-p13-crotty.pdf , the OS is generally not optimised for paging out the correct pages for database access patterns

---

With all of the above in mind, here are my answers to the points in 12-11-25.md:

1. u64
2. big endian
3. binary struct, capnproto schemas for node/edge properties (hopefully)
4. option 2, `(srcNodeId, typeId, dstNodeId, edgeId)` and `(dstNodeId, typeId, srcNodeId, edgeId)`
5. nah let's not do that that's scary
6. it's very difficult for me to know up front whether this would be useful/predict what issues it may cause in future. for now i think i'm going to keep nodes and edges as entirely separate concepts, but can revisit
7. let's do incremental int ids as discussed above
8. `nodeId, vectorTypeId -> vectorId` sounds good
9. don't worry about this for now
10. duplicate of 7
11. duplicate of 3

---

Update post-building all of the above:

- it seems to be working!!! I got capnproto working as a property schema format, keeping things as a library makes life super easy but the ocaml capnp reader/builder API is quite clunky to work with
- all the main operations (CRUD on nodes/edges, setting/getting properties) are working as expected, and zero-copy reads are working as expected (i think)
- next steps are:
  - a transaction api - need to figure out whether to have like manual begin/commit/rollback or have a with_transaction HOF
  - need to expose some way to scan all nodes/edges/vectors in the database
  - patch operations on properties, right now its put
  - filter adjacency queries by properties
  - write a full test suite
